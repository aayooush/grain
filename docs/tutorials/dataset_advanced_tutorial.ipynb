{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced `Dataset` usage\n",
    "\n",
    "If you decided to use `Dataset` APIs, there's a good chance you want to do one\n",
    "or more processing steps described in this section, especially if working on\n",
    "data ingestion for generative model training.\n",
    "\n",
    "```python\n",
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install grain\n",
    "# @test {\"output\": \"ignore\"}\n",
    "!pip install tensorflow_datasets\n",
    "```\n",
    "\n",
    "```python\n",
    "import grain.python as grain\n",
    "import tensorflow_datasets as tfds\n",
    "```\n",
    "\n",
    "## Checkpointing\n",
    "\n",
    "We provide `PyGrainCheckpoint{Save|Restore}` to checkpoint the\n",
    "`DatasetIterator`. It is recommended to use it with\n",
    "[Orbax](https://orbax.readthedocs.io/en/latest/index.html), which can checkpoint\n",
    "both, input pipeline and model, and handles the edge cases for distributed\n",
    "training.\n",
    "\n",
    "```python\n",
    "ds = (\n",
    "    grain.MapDataset.source(tfds.data_source(\"mnist\", split=\"train\"))\n",
    "    .seed(seed=45)\n",
    "    .shuffle()\n",
    "    .to_iter_dataset()\n",
    ")\n",
    "\n",
    "num_steps = 4\n",
    "ds_iter = iter(ds)\n",
    "\n",
    "# Read some elements.\n",
    "for i in range(num_steps):\n",
    "  x = next(ds_iter)\n",
    "  print(i, x[\"label\"])\n",
    "```\n",
    "\n",
    "```\n",
    "0 7\n",
    "1 4\n",
    "2 0\n",
    "3 1\n",
    "```\n",
    "\n",
    "```python\n",
    "!pip install orbax\n",
    "```\n",
    "\n",
    "```python\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "mngr = ocp.CheckpointManager(\"/tmp/orbax\")\n",
    "\n",
    "!rm -rf /tmp/orbax\n",
    "\n",
    "# Save the checkpoint.\n",
    "assert mngr.save(\n",
    "    step=num_steps, args=grain.PyGrainCheckpointSave(ds_iter), force=True\n",
    ")\n",
    "# Checkpoint saving in Orbax is asynchronous by default, so we'll wait until\n",
    "# finished before examining checkpoint.\n",
    "mngr.wait_until_finished()\n",
    "\n",
    "!ls -R /tmp/orbax\n",
    "```\n",
    "\n",
    "```\n",
    "/tmp/orbax:\n",
    "4\n",
    "\n",
    "/tmp/orbax/4:\n",
    "_CHECKPOINT_METADATA\n",
    "default\n",
    "descriptor\n",
    "\n",
    "/tmp/orbax/4/default:\n",
    "process_0-of-1.json\n",
    "\n",
    "/tmp/orbax/4/descriptor:\n",
    "descriptor.pbtxt\n",
    "```\n",
    "\n",
    "```python\n",
    "!cat /tmp/orbax/*/*/*.json\n",
    "```\n",
    "\n",
    "```\n",
    "{\n",
    "    \"next_index\": 4\n",
    "}\n",
    "```\n",
    "\n",
    "```python\n",
    "# Read more elements and advance the iterator.\n",
    "for i in range(4, 8):\n",
    "  x = next(ds_iter)\n",
    "  print(i, x[\"label\"])\n",
    "```\n",
    "\n",
    "```\n",
    "4 7\n",
    "5 4\n",
    "6 8\n",
    "7 0\n",
    "```\n",
    "\n",
    "```python\n",
    "# Restore iterator from the previously saved checkpoint.\n",
    "mngr.restore(num_steps, args=grain.PyGrainCheckpointRestore(ds_iter))\n",
    "# Iterator should be set back to start from 4.\n",
    "for i in range(4, 8):\n",
    "  x = next(ds_iter)\n",
    "  print(i, x[\"label\"])\n",
    "```\n",
    "\n",
    "```\n",
    "4 7\n",
    "5 4\n",
    "6 8\n",
    "7 0\n",
    "```\n",
    "\n",
    "## Mixing datasets\n",
    "\n",
    "`Dataset` allows mixing multiple data sources with potentially different\n",
    "transformations. There's two different ways of mixing `Dataset`s:\n",
    "`MapDataset.mix` and `IterDataset.mix`. If the mixed `Datasets` are sparse (e.g.\n",
    "one of the mixture components needs to be filtered) use `IterDataset.mix`,\n",
    "otherwise use `MapDataset.mix`.\n",
    "\n",
    "```python\n",
    "import pprint\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "```python\n",
    "tfds.core.DatasetInfo.file_format = (\n",
    "    tfds.core.file_adapters.FileFormat.ARRAY_RECORD\n",
    ")\n",
    "# This particular dataset mixes medical images with hand written numbers,\n",
    "# probably not useful but allows to illustrate the API on small datasets.\n",
    "source1 = tfds.data_source(name=\"pneumonia_mnist\", split=\"train\")\n",
    "source2 = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds1 = grain.MapDataset.source(source1).map(lambda features: features[\"image\"])\n",
    "ds2 = grain.MapDataset.source(source2).map(lambda features: features[\"image\"])\n",
    "ds = grain.MapDataset.mix([ds1, ds2], weights=[0.7, 0.3])\n",
    "print(f\"Mixed dataset length = {len(ds)}\")\n",
    "pprint.pprint(np.shape(ds[0]))\n",
    "```\n",
    "\n",
    "```\n",
    "Mixed dataset length = 6728\n",
    "(28, 28, 1)\n",
    "```\n",
    "\n",
    "If filtering inputs to the mixture, use `IterDataset.mix`.\n",
    "\n",
    "```python\n",
    "source1 = tfds.data_source(name=\"pneumonia_mnist\", split=\"train\")\n",
    "source2 = tfds.data_source(name=\"mnist\", split=\"train\")\n",
    "ds1 = (\n",
    "    grain.MapDataset.source(source1)\n",
    "    .filter(lambda features: int(features[\"label\"]) == 1)\n",
    "    .to_iter_dataset()\n",
    ")\n",
    "ds2 = (\n",
    "    grain.MapDataset.source(source2)\n",
    "    .filter(lambda features: int(features[\"label\"]) > 4)\n",
    "    .to_iter_dataset()\n",
    ")\n",
    "\n",
    "ds = grain.IterDataset.mix([ds1, ds2], weights=[0.7, 0.3]).map(\n",
    "    lambda features: features[\"image\"]\n",
    ")\n",
    "pprint.pprint(np.shape(next(iter(ds))))\n",
    "```\n",
    "\n",
    "```\n",
    "(28, 28, 1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
