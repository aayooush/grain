{
  "cells": [
    {
      "metadata": {
        "id": "X8GEODAU3nic"
      },
      "cell_type": "markdown",
      "source": [
        "# Reading ArrayRecord Files\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/grain/blob/main/docs/tutorials/data_sources/arrayrecord_data_source_tutorial.ipynb)\n",
        "\n",
        "This tutorial provides an example of how to retrieve records from ArrayRecord files using `grain.sources.ArrayRecordDataSource`, also covers how to process and transform the data with Grain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p6V-crf3t-K"
      },
      "source": [
        "## Install and Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzWZLNklr4Iy"
      },
      "outputs": [],
      "source": [
        "!pip install grain array_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NF4E-cCbyjV"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import grain\n",
        "import tensorflow_datasets as tfds\n",
        "from array_record.python import array_record_module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBwdOjDn3t-K"
      },
      "source": [
        "## Write a temp ArrayRecord file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrCQ-jH53t-K"
      },
      "outputs": [],
      "source": [
        "# Load a public tensorflow dataset.\n",
        "test_tfds = tfds.data_source(\"bool_q\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0yBaN7hXmbu"
      },
      "outputs": [],
      "source": [
        "# Write the dataset into a test array_record file.\n",
        "example_file_path = \"./test.array_record\"\n",
        "writer = array_record_module.ArrayRecordWriter(\n",
        "    example_file_path, \"group_size:1\"\n",
        ")\n",
        "record_count = 0\n",
        "for record in test_tfds:\n",
        "  writer.write(pickle.dumps(record))\n",
        "  record_count += 1\n",
        "writer.close()\n",
        "\n",
        "print(\n",
        "    f\"Number of records written to array_record file {example_file_path} :\"\n",
        "    f\" {record_count}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKJ_49JCXmbu"
      },
      "outputs": [],
      "source": [
        "# @title Load Data Source\n",
        "example_array_record_data_source = (grain.sources.ArrayRecordDataSource(\n",
        "    example_file_path\n",
        "))\n",
        "print(f\"Number of records: {len(example_array_record_data_source)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVRGllY3Xmbu"
      },
      "outputs": [],
      "source": [
        "print(example_array_record_data_source[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2nXJLVUXmbu"
      },
      "source": [
        "## Define Transformation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AS5w9quXmbu"
      },
      "outputs": [],
      "source": [
        "# Load a pre trained tokenizer\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiS85paBXmbu"
      },
      "outputs": [],
      "source": [
        "class ParseAndTokenizeText(grain.transforms.Map):\n",
        "  \"\"\"This function takes a serialized dict (as bytes), decodes it,\n",
        "\n",
        "  applies a tokenizer to a specified feature within the dict,\n",
        "  and returns the first 10 tokens from results.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, tokenizer, feature_name):\n",
        "    self._tokenizer = tokenizer\n",
        "    self._feature_name = feature_name\n",
        "\n",
        "  def map(self, element: bytes) -\u003e [str]:\n",
        "    parsed_element = pickle.loads(element)\n",
        "    # only pick the first 10 token IDs from the tokenized text for testing\n",
        "    return self._tokenizer.encode(\n",
        "        parsed_element[self._feature_name].decode('utf-8')\n",
        "    ).tokens[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLqi3i7O3t-K"
      },
      "source": [
        "## Load and process data via the Dataset API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPIy05gGUBzI"
      },
      "outputs": [],
      "source": [
        "# Example using Grain's MapDataset with ArrayRecord file source.\n",
        "example_datasets = (\n",
        "    grain.MapDataset.source(example_array_record_data_source)\n",
        "    .shuffle(seed=42)\n",
        "    .map(ParseAndTokenizeText(tokenizer, \"question\"))\n",
        "    .batch(batch_size=10)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqJSeQ9hdAmF"
      },
      "outputs": [],
      "source": [
        "# Output a record at a random index\n",
        "print(example_datasets[100])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1JbSnqHDYHFt_nLm24U_xmtL8czPgUaU6",
          "timestamp": 1744760988378
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
